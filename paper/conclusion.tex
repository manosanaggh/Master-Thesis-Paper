\section{Conclusions}

In this thesis, we conducted an analysis of throughput for managed big data analytics frameworks
using Apache Spark and Giraph under workload co-location. We investigated, if reducing GC and S/D for managed big data frameworks improves application throughput by using an open-source system TeraHeap. We conducted our experiments under 3 different memory-per-core
scenarios, 4, 8 and 16 GB / core, in order to see if increasing memory capacity helps increasing server throughput. 4 GB / core is the current trend and 8 and 16 GB / core are possible future trends.
For simplicity, we divided total DRAM capacity to 2,4 and 8 even memory budgets. We used each budget to run each instance isolated with Native Spark and Giraph and Spark and Giraph using TH to study the execution breakdown.
Then we run experiments with 2,4 and 8 co-located instances using the above budgets for each instance. We ran 4 Spark workloads (PR, LinR, LogR and CC) in the 4 and 8 GB / core scenario and 2 Giraph workloads (PR, CDLP) in the 8 and 16 GB / core scenario. We ran Giraph under 16 GB / core, because it is more memory intensive than Spark. We reported interference with single instance, execution breakdown (GC, S/D, I/O), user and CPU utilization, CPU cycles and average throughput. We also included a cost estimation of the experiments in several public clusters to show that decreasing GC and S/D helps utilizating monetary budgets for renting servers more effectively.

Our experimental results showed that reducing GC and S/D for Spark reduces execution time and increases the effective CPU utilization by the applications threads, where in Giraph that assumption is not confirmed. Furthermore, decreasing GC and S/D allows a higher number of co-located instances to be executed in the server, because of lower memory per instance needs. Overall, our analysis showed that high CPU utilization does not always mean that useful work is done by the CPU. Specificaly for managed
big data frameworks like Spark and Giraph a lot of CPU cycles are wasted on GC and S/D and even increasing H1 by increasing memory-per-core does not guarantee optimal execution.

