\section{Related Work}

\iffalse
To address the challenges of lack of system memory, researchers have proposed various
techniques to optimize the performance of Big data analytics, including
utilizing remote memory, memory scheduling, and offloading to local storage.

Some Meta researchers built a new memory scheduling mechanism to investigate the oportunities
for reducing memory pressure and improve server throughput in their work called Transparent Memory Offloading in Datacenters (TMO)
\cite{TMO}. Amaro et. al investigate the utilization of remote memory to improve job throughput \cite{CFM} building a swapping mechnism that uses memory
from remote servers as swap memory.
\fi

\note{jk: Try to categorize the related work and add citations using
"cite\{\} command". Try to use the name of the system in each paper
and not authors name. When you discuss the work in each category try
to show paper in chronological order.}

Several studies have been conducted to improve the performance of big
data processing systems based on the memory pressure that arises. One approach is to utilize memory-model-based task
co-location during runtime to improve Spark application throughput, which has been
investigated by Marco et al. in \cite{Colocation}. They choose a variety of memory-models
that describe the memory footprint of Spark applications and determine the right memory-model
for the specific application at runtime. That way they learn how many applications can be colocated
to achieve better throughput.
Meanwhile, in \cite{Limits}, the authors investigate the trade-off between lower Garbage Collection overhead and memory usage. The authors propose MemBalancer, a compositional square-root heap limit rule for the
V8 JavaScript engine that achieves low GC times while also decreasing the browser memory usage.
In cloud computing platforms, Sharma
et al. proposed per-VM page cache partitioning in \cite{PC} 
to get rid-off the interferences that arise with the shared Hypervisor Page Cache.
\iffalse
\note{jk: is this related of what we
are doing? Maybe yes but you need to provide more context}. Additionally, Bhimani et al. proposed
a lightweight virtualization framework for accelerating big data
applications on enterprise cloud in [10], while Zhang et al. focused
on understanding and improving disk-based intermediate data caching in
Spark in [11]. Finally, Intasorn et al. investigated using compression
tables to improve HiveQL performance with Spark in a case study on
NVMe storage devices in [12]. 

\note{I thin this paragraph does not help. It sounds like the previous
study make a big effort and provide analysis. So, I guess, why do I
need to read this paper? What is the new that I will learn. Can we
show the open research questions/problems that the previous studies do
not target?}
These studies demonstrate a variety of approaches for optimizing big
data processing systems, ranging from memory-aware task co-location
and memory offloading to scheduler design and virtualization
frameworks. The findings from these studies can provide insights and
guidance for future research in the field of big data processing.
\fi
