\section{Related Work}

We group the related work in the two following categories:
\begin{itemize}
\item{Works that examine collocation of workloads}
\item{Other analyses on managed big data frameworks}
\end{itemize}

\subsection{Work that examine the collocation of workloads}
To our best knowledge there is limited work in investigating workload co-location. Here we refer to some works does in this area.


Baig et al. in \cite{NUMA} investigate how Spark-based workloads are impacted by the effects of NUMA-placement decisions. This is something we do not do in our work, because we run our experiments in a single NUMA island to avoid NUMA effects that could complicate the understanding of GC, S/D and the aspects of execution that we investigate. Apart from that difference they investigate the performance of co-located spark workers where each worker runs in a different NUMA island. They count remote memory accesses and context switches in CPU. Chen et al. in \cite{interference} analyze the characteristics of co-located workloads running in containers on the same server from the perspective of hardware events. These events include inctructions per cycle, branch prediction misses and dTLB misses. They also show the execution time of co-located workloads, but they do not provide further analysis or breakdown.

\subsection{Other analyses on managed big data frameworks}

Here we refer to other evaluation works targetting managed big data frameworks.
These works do not provide analyses for workload co-location.

Jiang et al. in \cite{inmem} study the behavior of Spark Workloads in comparison to those of Giraph, CloudSuite, SPEC CPU2006,
TPC-C, and DesktopCloud on system (i.e. disk utilization, memory bandwidth)  and microarchitectural level (instructions per cycle). This work
also provides an analysis for Spark and Giraph examining the behaviour from a different scope than ours. However, it does not provide a breakdown to the execution time of the workloads (i.e. GC, S/D) or CPU utilization analysis.
Ousterhout et al. \cite{makingsense} provide a methodology based on dynamic logging and profiling for quantifying performance bottlenecks in distributed computation frameworks, and use it to analyze the Sparkâ€™s performance. They refer and measure S/D, GC and CPU utilization but they don't refer to co-located workloads or target other frameworks. Batarfi et al. \cite{giraphgraphalytics} analyze the performance of many graph processing frameworks including Giraph. They provide results on RAM usage, CPU utilization and execution time. However, they investigate a different aspect from execution time. They break it down to the time taken by each phase of the workload execution.  They also only show results for graph processing and do not target other areas like machine learning as we do. Furthermore, their work is evaluated only aganst Spark. 

