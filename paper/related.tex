\section{Related Work}

Several studies have been conducted to improve the performance of big
data processing systems. One approach is to utilize memory-aware task
co-location to improve Spark application throughput, which has been
investigated by Marco et al. in [3]. Meanwhile, in [4], Kirisame et
al. proposed optimal heap limits to reduce browser memory use. Another
research direction is to leverage far memory to improve job
throughput, as studied by Amaro et al. in [5]. To facilitate memory
offloading in datacenters, Weiner et al. presented TMO, a transparent
memory offloading system in [6]. In cloud computing platforms, Sharma
et al. proposed per-VM page cache partitioning to improve performance
in [7]. Chen and Wang introduced Spark on Entropy, a reliable and
efficient scheduler for low-latency parallel jobs in heterogeneous
clouds, in [8]. Thamsen et al. developed Mary, Hugo, and Hugo*, three
learning-based schedulers for distributed data-parallel processing
jobs on shared clusters in [9]. Additionally, Bhimani et al. proposed
a lightweight virtualization framework for accelerating big data
applications on enterprise cloud in [10], while Zhang et al. focused
on understanding and improving disk-based intermediate data caching in
Spark in [11]. Finally, Intasorn et al. investigated using compression
tables to improve HiveQL performance with Spark in a case study on
NVMe storage devices in [12]. 

These studies demonstrate a variety of approaches for optimizing big
data processing systems, ranging from memory-aware task co-location
and memory offloading to scheduler design and virtualization
frameworks. The findings from these studies can provide insights and
guidance for future research in the field of big data processing.
