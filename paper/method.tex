\section{Experimental Methodology}
\label{sec:method}

In this section we discuss our methodological decisions.

Our methodology answers the following questions:
\begin{itemize}
	\item{What workloads did we choose to run for our experiments and why?}
	\item{How do we investigate the memory per core problem?}
	\item{How do we choose the configurations for running the co-located experiments?}
	\item{Is cost a contributing factor to pursuing higher throughput for a server?}
\end{itemize}

\subsection{Workloads}
For our experiments with Spark, we selected four specific workloads from two
different categories of the Spark Bench suite \cite{Spark-Bench}: Page Rank (PR) and Connected
Component (CC) from GraphX \cite{GraphX} and Linear Regression (LinR) and Logistic Regression (LogR)
from MLLib \cite{MLLib}. For Giraph, we choose PageRank and Community detection 
using label propagation (CDLP) from LDBC Graphalytics \cite{ldbc}. The primary reason for selecting these workloads for Spark is that
they represent different types of algorithms: PR and CC are graph-based workloads, while LinR and LogR are machine learning
workloads. Giraph is a graph processing framework so we only used graph workloads. All of these
workloads are well-established and commonly used for benchmarking big
data analytics systems, making them a suitable choice for our
experiments. Overall, the selection of these workloads allows us to
evaluate the performance of Spark and Giraph in a variety of contexts. Furthermore, it allows us to
provide insights into the performance of both frameworks with or without using TeraHeap.


\subsubsection{PageRank}
PageRank is a widely used graph-based algorithm that measures the
importance of nodes in a network. It has become a popular benchmark
for evaluating the performance of distributed systems, including big
data analytics systems like Apache Spark and Giraph. PageRank is computationally
intensive and requires significant memory and I/O resources, making it
a suitable workload for evaluating performance of managed big data frameworks. Additionally, PageRank is a
common algorithm in real-world applications, such as search engines
and social networks, making it relevant for practical use cases.

\subsubsection{LinearRegression}
LinearRegression is a machine learning algorithm that is used to
predict numerical values based on input data. It is a well-known and
widely used algorithm in machine learning, and is commonly used for
regression analysis in fields such as economics, finance, and
engineering. LinearRegression is computationally intensive and
requires significant memory and I/O resources, making it a suitable
workload for evaluating the performance of managed big data frameworks.

\subsubsection{Logistic Regression}
LogisticRegression is a machine learning algorithm that is used to
model the probability of a binary or categorical outcome based on one
or more independent variables. It is commonly used in predictive
analytics to classify data based on historical data. In Spark-bench,
LogisticRegression is implemented as a machine learning workload,
where the dataset is represented as an RDD of feature vectors and
labels. The LogisticRegression workload involves training a logistic
regression model on the dataset, using an iterative optimization
algorithm such as gradient descent. The workload is computationally
intensive and requires a significant amount of memory to store the
dataset and model parameters, therefore a suitable choise for our experiments..

\subsubsection{Connected Component}
ConnectedComponent is a graph algorithm that is used to identify the
connected components of a graph. It is commonly used in social network
analysis to identify clusters of users with similar interests or
relationships. In Spark-bench, ConnectedComponent is implemented as a
graph processing workload, where the graph is represented as an RDD of
edges and vertices. The ConnectedComponent workload involves iterating
over the graph, identifying the connected components of each node, and
merging the components as necessary. The workload is computationally
intensive and requires a significant amount of memory to store the
graph, therefore a suitable choise for our experiments.

\subsubsection{Community Detection Label Propagation}
The Community Detection using Label Propagation (CDLP) workload, another key component of the Graphalytics benchmark, aims to identify communities within a graph based on label propagation techniques. The CDLP workload assigns labels to nodes iteratively, with each node adopting the most frequently occurring label among its neighbors. This iterative process propagates labels throughout the graph, eventually converging to stable communities. Community detection is a fundamental task in graph analysis, enabling researchers to uncover groups of nodes that exhibit strong internal connectivity. It has applications in social network analysis, recommendation systems, and anomaly detection. The CDLP workload in the Graphalytics benchmark provides a standardized evaluation of graph processing systems' performance in terms of community detection scalability, convergence, and accuracy. By benchmarking CDLP, researchers and practitioners can compare the efficiency and effectiveness of different graph processing platforms and algorithms for community detection tasks.

\subsection{Memory per core}

\begin{table}[thbp]
  \centering
  \caption{Configurations. WL = workload, FW = framework, DS = dataset, Mem.= total memory, M/C = memory per core, Phys. Cores = physical cores}
  \label{tab:setups}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
	  \textbf{WL} & \textbf{FW} & \textbf{DS (GB)} & \textbf{Mem.} & \textbf{M/C} & \textbf{\#Phys. Cores}\\
    \hline
	  PR & Spark & 8 & 32 & 4 & 8 \\
	  PR & Spark & 8 & 64 & 8 & 8 \\
	  LinR & Spark & 64 & 32 & 4 & 8 \\
	  LinR & Spark & 64 & 64 & 8 & 8 \\
	  LogR & Spark & 64 & 32 & 4 & 8 \\
	  LogR & Spark & 64 & 64 & 8 & 8 \\
	  CC & Spark & 8 & 32 & 4 & 8 \\
	  CC & Spark & 8 & 64 & 8 & 8 \\
	  PR & Giraph & 13 & 64 & 8 & 8 \\
	  CDLP & Giraph & 13 & 64 & 8 & 8 \\
	  PR & Giraph & 13 & 128 & 16 & 8 \\
	  CDLP & Giraph & 13 & 128 & 16 & 8 \\
    \hline
  \end{tabular}
\end{table}

We investigate performance in three memory per core scenarios to check if throughput increases as memory availability increases. Our main focus is 4 GB per core which is the next possible trend in datacenters based on \ref{sec:intro}. The others
are 8 and 16 GB per core which is a possible future trend. For the 4 GB per core scenario, we use 32 GB memory with 8 cores.
In this setup, we run 4 workloads with Native Spark and Spark using TH. Giraph can't run with 4 GB per core with any of the two
configurations. For 8 GB per core, we use 64 GB memory with 8 cores. 
In this setup, we run 4 workloads with Native Spark and Spark using TH and 2 workloads with Native Giraph and Giraph using TH. For the 16 GB memory per core, we use 128 GB of memory with 16 cores. For this setup, we run 2 workloads with Native Giraph and Giraph using TeraHeap. We choose 16 GB per core for Giraph, because it experiences more memory pressure than Spark and it cannot even run with 4 GB DRAM per core. For 8 GB memory per core we are able to run only a few experiments with TH. This happens, because it does not have a very aggresive memory offloading mechanism and cannot offload the heap properly. Table \ref{tab:setups} summarizes all setups with the coresponding workloads and datasets.

\subsection{Choosing the configurations to run the co-located experiments}
To utilize all the available DRAM of the machine, we choose a simple method. 
We divide the total DRAM of the machine by the number of co-located workloads we run for each experiment.
For simplicity and time management, we choose the numbers 1,2,4,8. However, there are 2 problems with that decision
than we need to overcome. The first is that we need a number that is exactly divided by these numbers to give the same amount of DRAM to
all cgroups. The second is that we need to leave memory to the OS for system tasks that are executed along with the system reserved memory of 2 GB. For the 4 GB memory per core, we utilize 24 out of 32 GB total DRAM and leave the rest 8 GB to the OS for system reserved memory and system tasks. For the 8 GB memory per core scenario, we utilize 56 GB out of 64 GB total DRAM and leave the rest 8 GB to the OS. For the 16 GB memory per core scenario we utilize 120 out of 128 GB total DRAM and leave the rest 8 GB to the OS. 
In all scenarios, we choose the number closer to the total DRAM. After having perfomed these calculations, we run each experiment isolated and break down the execution time to know how each experiment performs isolated. Then we run the co-located experiments and study the interference in execution. For the co-located experiments, we run the same workload with the same dataset size for all instances. We do this for simplicity of explaining the results. For all experiments, we disable swap memory.
We use cgroups \cite{cgroups} to restrict the DRAM for all processes in a single instance of Spark and Giraph.
For cgroups, we choose as a baseline an 80\% of total cgroup DRAM budget for H1 as RedHat does for its cgroup containers from June 2023 \cite{redhatblog}.
The rest 20\% remains to the OS to be used as Page Cache.
For TeraHeap, we also run experiments with 40\% for H1 to investigate what happens when Page Cache dominates H1.
In some experiments TeraHeap requires more than 20\% for OS so we make an adjustment to the cgroup budget.
For Native Spark and Giraph, we do not report results for those experiments as we saw that Page Cache adjustments make no difference.

\subsection{Cost estimation}
Renting servers is a common practice for organizations requiring
computational resources, and the question arises as to whether
reducing the monetary cost is possible by achieving higher throughput
and faster workload completion. The relationship between cost
reduction and achieving higher throughput on rented servers is indeed
significant. By optimizing server performance, efficiently utilizing
resources, implementing workload scheduling, and improving
productivity, organizations can realize cost savings. Achieving higher
throughput and faster workload completion can lead to a reduced rental
duration, minimizing the time and associated costs of server usage.
Efficient resource utilization and workload scheduling contribute to
cost reduction by minimizing the number of servers required and
maximizing their utilization. Rental pricing models that take into
account resource utilization or data processed can further reduce
costs for organizations achieving higher throughput. Additionally,
improved productivity resulting from higher throughput and faster
workload completion enhances overall efficiency, allowing
organizations to accomplish more work within the same rental period
and reducing rental expenses. Therefore, pursuing higher throughput
and faster workload completion offers tangible benefits in terms of
monetary cost reduction for organizations renting servers. 

We estimate the cost of our experiments in real-world public clusters, to show that increasing throughput by
decreasing GC and S/D leads to avoiding wasting money in overheads when renting servers. We
chose a variety of providers like Amazon \cite{EC2}, Google \cite{GCP} and Microsoft \cite{Azure}. This
way, we covered the most known providers and platforms someone would
choose to run their workloads on. We chose 3 machines from each
platform identical to the specifications of our 32, 64 and 128 GB DRAM
machines. These are the cheapest machines of that particular category
offered by the platform. We then used the platform's pricing
calculator to estimate the cost of renting that machine for the time
needed for each configuration to finish execution of all instances. We noticed, that the price for
renting the storage device is really amenable to the cost for renting the machine.
%
