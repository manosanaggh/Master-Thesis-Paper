\section{Introduction}
\label{sec:intro}

With the exponential growth of data in various fields such as
healthcare and social media, managed big data frameworks (e.g, Spark \cite{Spark},
Giraph, and Flink) require large amount of DRAM. 
These frameworks, during processing, generate large amount of objects
in the managed heap that span multiple computation stages.
The memory pressure that arises in the managed heap leads to long frequent garbage
collection (GC) cycles.
%memory ressure -> GC
%large part of CPU utilization goes to GC.
%not useful work
%This
%requirement leads to excessive CPU under-utilization, leading to
%performance bottlenecks and slow job completion times.
%data -> memory -> reduce GC -> memory-per-core bound -> CPU under-utilization 

To reduce the frequency of GC, the heap capacity increases to withstand the object load.
Along with GC, the number of managed big data analytics frameworks instances increase
in the server to match the increased needs for data processing utilizing all available DRAM. The result of this practice
is that the underlying machine runs out of memory, while the GC overhead is still high. 
Furthermore the memory resources in datacenters are low compared to the available CPU cores.
This leads to the problem where some cores stay idle, because the memory/core is not enough.

The memory-per-core problem can better be understood when looking at the
resource usage and the characteristics of the servers of big companies e.g. Alibaba and Facebook.
When looking at the results of an Alibaba trace analysis \cite{Alibaba}, we see that memory usage
is at an average of 80\%, while CPU usage stays at 40\%. This trace clearly shows that DRAM utilization
is high, while the CPU is under-utilized. In Facebook's Twine presentation \cite{Twine},
they used a cluster of machines where each machine had 40 cores and 80 GB DRAM. This
means that memory/core was 2 GB which shows that memory capacity is not enough and most of the time many
the CPU cores are going to be idle.

To address the problem of DRAM capacity limitation, recent work
proposed solutions that extend the managed heaps over local flash
storage devices (e.g., NVMe SSD) or remote memory. On the one hand,
TMO \cite{TMO} offloads cold memory to fast storage devices using
a memory scheduling mechanism. On the other hand, CFM \cite{CFM}
utilizes remote DRAM as swap memory in order to increase total memory capacity
and reduce memory pressure. Of both works, only TMO shows
evaluation against managed big data analytics frameworks without
targetting the core obstacles like GC.

This paper provides a methdological analysis of server throughput 
focused on managed big data analytics frameworks.
We investigate the off-heap direction of offloading the heap
to fast storage devices.
Specifically, we use TeraHeap \cite{TeraHeap}, a secondary managed
memory-mapped heap over an NVMe storage device, which is used to hold
the long lived objects instead of the main managed Java Heap. TeraHeap
1) eliminates Serialization/Deserialization overheads posed by this
kind of frameworks when moving data off-heap to/from fast storage
devices 2) reduces GC pauses drastically over the secondary heap. By
offloading the managed Java Heap and relaxing computation-intensive
tasks, we aim to reduce the load of Garbage Collection on executors,
thereby improving their performance and job throughput. We investigate
the effectiveness of this approach using various big
data analytics workloads on a real-world cluster. We choose the
parameters for running the workloads by investigating the needs of big
data analytics in memory budget. We define memory budget as
the summary of Java Heap, IO Cache and JVM native memory. We compare
TeraHeap with the native Spark and Giraph distributions and show the potential of
an approach like TeraHeap to improve performance and server throughput.

Our experimental results show that reducing Java Garbage Collection
overhead by offloading the heap to fast storage devices significantly
improves server throughput by up to 66\% against native Spark. 
Finally, we also include a cost estimation to show that
TeraHeap could reduce monetary cost by up to 50\% for running big data
analytics, if deployed in a world cluster like Amazon's EC2 or Google
Cloud Platform or Microsoft Azure Cloud, which are available to
everyone.

To summarize, this paper makes the following contributions: 
\begin{itemize}
    \item{A detailed methodology for running Apache Spark and Giraph using TeraHeap
	  as a heap offloading mechanism.}
    \item{A comprehensive evaluation of the performance and cost trade-offs of TeraHeap against Native Spark-Giraph
	  with a single or multiple colocated executors in the same server.
    \item{A cost estimation of running our experiments in real-world cloud platforms like Amazon EC2, Google Cloud and Microsoft Azure.}
\end{itemize}
