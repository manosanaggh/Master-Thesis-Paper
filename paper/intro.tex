\section{Introduction}
\label{sec:intro}

With the exponential growth of data in various fields such as
healthcare and social media, managed big data frameworks (e.g, Apache
Spark \cite{Spark} and Apache Giraph \cite{Giraph}) require large
amount of DRAM. These frameworks, during processing, generate large
amount of objects in the managed heap that span multiple computation
stages. The memory pressure that arises in the managed heap leads to
frequent garbage collection (GC) cycles. Frequent GCs waste CPU cycles 
and prevent application execution.

To reduce the frequency of GC and optimize performance, users of big data
frameworks try two different approaches. First, they increase heap capacity to
facilitate the big number of allocated objects. Apart from increasing
heap size, they use mechanisms offfered by the frameoworks 
to offload objects from the heap to storage devices. However, these
objects need to be serialized to byte streams to be stored in the storage
device or to be deserialized into memory objects to be loaded back to memory. 
This practice leads to high serialization/deserialization overhead.

Furthermore, due to the increased needs for data processing, there is
a need to co-locate many isolated managed big data framework
instances. Co-locating workloads increases available resource utilization
thus increasing the throughput in server. 
In order to maximize throughput, the number of instances
increase to utilize all available DRAM. The result of this
practice is that the underlying machine runs out of memory, while the
 overhead of GC and S/D is still high. The remaining GC and S/D
overheads lead to the problem of wasting the CPU resources. This
leads to the conclusion that the avalaible memory/core is
not enough for both the Garbage Collector and the application.

The memory-per-core problem can better be understood when looking at
the resource usage and the characteristics of the servers of big
companies e.g. Alibaba and Facebook. When looking at the results of
Alibaba's traces analyses (\cite{Alibaba}, \cite{Alibaba1},
\cite{Alibabacolocated}) we see that memory usage is at an average of
80\%, while CPU usage stays at 40\%. This trace clearly shows that
DRAM utilization is high, while the CPU is under-utilized. In
Facebook's Twine presentation \cite{Twine}, they used a cluster of
machines where each machine had 40 cores and 80 GB DRAM. This means
that ratio of GB for memory/core was 2. This shows that memory
capacity for each core is low. Most of the time many the CPU cores are
going to be idle because a few of them will be enough to carry out the
work.

To address the problem of DRAM capacity limitation, recent work
proposed solutions that extend the managed heaps over local flash
storage devices (e.g., NVMe SSD) or remote memory. On the one hand,
TMO \cite{TMO} offloads cold memory to fast storage devices using
a memory scheduling mechanism. On the other hand, CFM \cite{CFM}
utilizes remote DRAM as swap memory in order to increase total memory capacity
and reduce memory pressure. Of both works, only CFM shows
evaluation against managed big data analytics frameworks. However, this evaluation
includes only one Spark workload and is not focused on analytics.

This paper provides a methodological analysis of server throughput 
focused on managed big data analytics frameworks.
We investigate the off-heap direction of offloading the objects from the
managed heap to fast storage devices.
Specifically, we use TeraHeap \cite{TeraHeap}, a secondary managed
memory-mapped heap over an NVMe storage device, which is used to hold
the long lived objects instead of the main managed Java Heap. TeraHeap
1) eliminates Serialization/Deserialization overheads posed by this
kind of frameworks when moving data off-heap to/from fast storage
devices 2) reduces GC pauses drastically over the secondary heap. By
using TeraHeap, we aim to investigate the impact of reducing GC and S/D
to server throughput under workload co-location. We evaluate
the effectiveness of this approach by running 2 widely used
managed big data frameworks, Apache Spark and Giraph. We
specificaly run 4 workloads for Spark and 3 workloads for Giraph.
We choose the optimal configuration for running these workloads by investigating the needs of both
frameworks in memory budget. We define memory budget as
the summary of Java Heap, IO Cache and JVM native memory. We compare
TeraHeap with the native Spark and Giraph distributions under workload
co-location and analyze their performance using several metrics like
GC, S/D, I/O and CPU utilization. Finally, we estimate the cost of running these
experiments in public world clusters like Amazon EC2, Google Cloud Platform (GCP)
and Microsoft Azure Cloud.

Our experimental results show that reducing Java Garbage Collection
overhead and S/D by offloading the heap to fast storage devices significantly
improves server throughput by up to 66\% against native Spark. 
Finally, we also include a cost estimation to show that
TeraHeap could reduce monetary cost by up to 50\% for running big data
analytics, if deployed in a world cluster like EC2, GCP or Microsoft Azure Cloud, which are available to
the public.

To summarize, this paper makes the following contributions: 
\begin{itemize}
    \item{A detailed methodology for running co-located Apache Spark and Giraph
        workloads with or without TeraHeap.
	We explore the memory needs of each offloading approach in Java Heap and IO Cache. We then use 
	the optimal setup to run the max number of co-located workloads to utilize all available DRAM.}

    \item{A comprehensive evaluation of both Spark and Giraph with or without the use TeraHeap.
	    We run 4 Spark and 3 Giraph workloads,
	    analyzing different aspects of performance like GC, S/D, I/O
		and CPU utilization.}

    \item{A cost estimation of running our experiments in real-world
        cloud platforms like Amazon EC2, Google Cloud and Microsoft
        Azure.}
\end{itemize}
