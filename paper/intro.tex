\section{Introduction}
\label{sec:intro}

With the exponential growth of data in various fields such as finance,
healthcare, social media, and e-commerce, there is a significant need
for scalable and efficient big data processing frameworks. 
\note{jk: Add citations}
Apache
Spark \cite{Spark} is a framework that has gained popularity
due to its ability to handle large-scale data processing and
analytics. Spark provides a distributed computing platform that can
process data in parallel across multiple nodes in a cluster. However,
with the increasing size and complexity of big data workloads, Spark
clusters are facing significant challenges in meeting the performance
and throughput requirements. \note{jk: I do not understand this
paragraph. What is the problem. The  data or the Spark? The first
sentence of each paragraph provides the context of the paragraph.}

One of the main challenges in Spark clusters \note{jk: is not only in
Spark clusters. We do not write a Spark paper. Giraph or Neo4j also
face memory pressure issues} is the high computational
and memory requirements of big data analytics workloads. \note{Explain
in more precise way the computational needs e.g. caching.} These
requirements can result in excessive CPU and memory usage on Spark
workers, leading to performance bottlenecks and slow job completion
times. To address these challenges, researchers have proposed various
techniques \note{jk: add citations} to optimize the performance of Spark clusters, including
data partitioning, caching, and resource allocation.

\note{jk: explain why you only focus on the memory limit}
In this paper, we focus on the memory limit problem of servers
becoming an obstacle for further throughput increase and we propose a
new technique \note{jk: you do not propose a tecnique. You make
evaluation} for improving the performance and job throughput of
Spark clusters by moving long-lived objects from the main managed Java
Heap to a fast storage device such as NVMe, thereby saving memory for
other more useful tasks and leaving space for more CPU utilization.
Our approach leverages the capabilities of the underlying machine to
create less memory-consuming computation tasks, thereby reducing the
workload on the Spark workers, while maintaining effective
per-executor performance under the colocation of multiple executors
required to achieve max throughput. \note{jk: Please use smaller
sentences.}

\note{jk: I think you need one or two paragraphs explaining about your
memthodology. What are you trying to investigate. Why your methodology
is important. And say why are you using TeraHeap. Why you do not use
other systems? Why TeraHepa is enough?}
Specifically, in order to achieve higher throughput and better
performance for Spark, we use TeraHeap, a secondary managed
memory-mapped heap over an NVMe storage device, which is used to hold
the Resilient Distributed Datasets (Spark RDDs) instead of the main
managed Java Heap and completely remove any
Serialization/Deserialization and Garbage Collection (GC) cost over
them.

TeraHeap 1) eliminates Serialization/Deserialization overheads posed
by this kind of frameworks when moving data off-heap to/from fast
storage devices 2) eliminates GC pauses over the secondary heap,
therefore significantly minimizing overall GC overhead. By offloading
the managed Java Heap and relaxing computation-intensive tasks, we aim
to reduce the workload on Spark workers, thereby improving their
performance and job throughput. We also explore the trade-offs between
the cost of offloading and the performance gains achieved.

We demonstrate the effectiveness of our approach using various big
data analytics workloads on a real-world Spark cluster. We also
compare our approach with the native Spark distribution and show that
our approach can be used instead of this distribution to improve
performance and server throughput. \note{Here you should provide some
overall results. Please update this paragraph including Giraph}

The paper makes the following contributions: 
\begin{itemize}
    \item{A comprehensive
        evaluation of the performance and cost trade-offs of our proposed heap
        offloading technique, that utilizes fast storage devices to improve
        server throughput for Apache Spark, against Native Spark.
        \note{jk: I think is not only Spark} }
    \item{A detailed methodology for running Apache Spark and
        understanding how to use it for conducting off-heap
        experiments using the Native Spark or TeraHeap \note{jk: this
        is similar with the first contirbution.}}
    \item{3) Proves that heap offloading to fast storage devices is a
        significant research direction. \note{jk: I think this is not
        a contribution.}}
\end{itemize}

\note{jk: Remove this paragraph. It is unnecessary.}
The rest of the paper is organized as follows. In section 2 we discuss
related work on Spark optimization techniques and offloading
techniques. In section 3, we describe our experimental methodology in
order for someone to achieve the desired performance using TeraHeap.
In section 4, we present our experimental results and evaluate the
performance and cost trade-offs of our approach. In section 5, we
discuss future research directions. Finally we conclude the paper in
section 6 with an outline of our work.

\note{jk: Overall. The first two paragraphs of the introduction should
show the problem statement. Then 3rd paragraph should summarize the
limitations of existing work. Next you should show what this paper
does. For exaple about methodology. Why it is important? What
questions are you trying to answer. Then in the next paragraph you
should try to summarize the overall results. Finally add the
contributions of the paper as an itemized list.}
