\section{Introduction}
\label{sec:intro}
With the exponential growth of data in various fields such as
healthcare and social media, managed big data frameworks (e.g, Apache
Spark \cite{Spark} and Apache Giraph \cite{Giraph}) require large
amount of DRAM per core for data processing. During the processing, they generate large
amount of objects in the managed heap that span multiple computation
stages. The memory pressure that arises in the managed heap leads to
frequent garbage collection (GC) cycles. Frequent GCs waste CPU cycles 
and prevent application execution.

On the one hand, to reduce the frequency of GC and optimize performance, big data
frameworks offload objects from the managed heap to storage devices. However, these
objects need to be serialized to byte streams to be stored in the storage
device or to be deserialized into memory objects to be loaded back to memory. 
This practice leads to high serialization/deserialization overhead.
On the other hand infrastructure providers, trying to address the same problem increase memory per framework instance that runs in the server. This leaves CPU cores underutilized. 

Co-locating workloads aims to increase available resource utilization
thus increasing the throughput in server. 
In order to maximize throughput, the number of instances
increase to utilize all available DRAM. The result of this
practice is that the underlying machine runs out of memory, while the
 overhead of GC and S/D is still high. The remaining GC and S/D
overheads lead to the problem of wasting the CPU resources
to do unuseful work. This leads to the conclusion that the avalaible memory per core is
not enough for the Garbage Collector, S/D and the application.

The memory per core problem can better be understood when looking at
the resource usage and the characteristics of the servers of big
companies e.g. Alibaba and Facebook. When looking at the results of
Alibaba's traces analyses (\cite{Alibaba}, \cite{Alibaba1},
\cite{Alibabacolocated}) we see that memory usage is at an average of
80\%, while CPU usage stays at 40\%. This trace clearly shows that
DRAM utilization is high, while the CPU is under-utilized. In
Facebook's Twine presentation \cite{Twine}, they used a cluster of
machines where each machine had 40 cores and 80 GB DRAM. This means
that ratio of GB for memory per core was 2. The same ratio is shown in Facebook's Yosemite
\cite{Yosemite}. This shows that memory
capacity for each core is low while DRAM usage is high compared to the CPU usage. Most of the time many the CPU cores are
going to be idle because a few of them will be enough to carry out the
work.

To address the problem of DRAM capacity limitation, recent work
proposed solutions that extend the managed heaps over local flash
storage devices (e.g., NVMe SSD) or remote memory. On the one hand,
TMO \cite{TMO} offloads cold memory to fast storage devices using
a memory scheduling mechanism. On the other hand, CFM \cite{CFM}
utilizes remote DRAM as swap memory in order to increase total memory capacity
and reduce memory pressure. Of both works, only CFM shows
evaluation against managed big data analytics frameworks. However, this evaluation
includes only one Spark workload and is not focused on analytics.

This thesis provides a methodological analysis of server throughput 
focused on managed big data analytics frameworks.
We investigate the off-heap direction of offloading the objects from the
managed heap to fast storage devices.
Specifically, we use TeraHeap (TH) \cite{TeraHeap}, a secondary managed
memory-mapped heap over an NVMe storage device, which is used to hold
the long lived objects instead of the main managed Java Heap. TeraHeap
1) eliminates Serialization/Deserialization overheads posed by this
kind of frameworks when moving data off-heap to/from fast storage
devices 2) reduces GC pauses drastically over the secondary heap. By
using TeraHeap, we aim to investigate the impact of reducing GC and S/D
to server throughput compared to Native Spark and Giraph. We divide all the available DRAM
in our machine to 2,4 and 8 even budgets to run experiments with co-located instances.
We do this to utilize all available DRAM and then check CPU utilization to understand throughput.
First, we run each instance isolated to analyze performance and be able to study the interference when adding more
co-located instances. We run each individual workload with a different Spark or Giraph instance in a cgroup.
We do this, to limit the memory budget for each instance. Memory budget is
the summary of Java Heap, IO Cache (Linux Page Cache) and JVM native memory. We choose
the Java Heap (H1) ratio over the total DRAM budget based on RedHat's decisions
for running containers as a baseline. We also run experiments with more Page Cache (PC) ratio than H1
to investigate Page Cache affection to the performance. We show performance of both Native Spark-Giraph and Spark-Giraph with TH in 3 different
memory per core scenarios, 4 GB per core, which is the current trend and 8 and 16 GB per core 
as possible future trends. We do this to study the changes to server throughput as memory-per-core increases. 
We evaluate both offloading techniques by running 2 widely used
managed big data frameworks, Apache Spark and Giraph. We
specificaly run 4 different workloads with Spark with 4 and 8 GB per core.
We run 2 different workloads with Giraph with 8 and 16 GB per core.
We compare TeraHeap with the native Spark and Giraph distributions under workload
co-location and analyze their performance using several metrics like
GC, S/D, I/O, CPU cycles and CPU utilization. Finally, we estimate the cost of running these
experiments in public world clusters like Amazon EC2, Google Cloud Platform (GCP)
and Microsoft Azure Cloud to see possible benefits of either of the two techniques.

Our experimental results show that increasing memory per core does not guarantee reaching max throughput for managed big data frameworks.
A solution is to move the managed heap over fast storage devices in order to offload objects like TeraHeap and Panthera \cite{Panthera}.
Furthermore, reducing GC and S/D by offloading the heap to fast storage devices improves effective CPU utlization up to 59\% in CPU cycles for Spark and also leaves place to run more co-located instances in the server for both frameworks.
Finally, we also include a cost estimation to show that reducing GC and S/D could reduce monetary spendings by up to 50\% for running big data
analytics, in a world cluster like EC2, GCP or Microsoft Azure Cloud, which are available to
the public.

To summarize, this thesis makes the following contributions: 
\begin{itemize}
    \item{A detailed methodology for running co-located Apache Spark and Giraph
        workloads with or without TeraHeap. 
		We show the interference impact of running multiple co-located managed big data frameworks workloads. We also show that, increasing DRAM capacity is not the solution to the problem of server throughput. First, DRAM density cannot scale further. Therefore, increasing memory-per-core allows more instances to run in the server, but the overheads of GC and S/D remain, because the heap size still is not enough. This leads to the conclusion that these overheads are the obstacle to reach max throughput. Moreover, decreasing GC and S/D, increases the number of co-located instances that can be executed in the server as well.}

    \item{A cost estimation for running our experiments in real-world
        cloud platforms like Amazon EC2, Google Cloud Platform and Microsoft
        Azure. This estimation shows that decreasing GC and S/D leads to less spendings, because money is not wasted to overheads.}
\end{itemize}

%
\end{itemize}
