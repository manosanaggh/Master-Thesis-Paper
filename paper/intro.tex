\section{Introduction}
\label{sec:intro}

With the exponential growth of data in various fields such as
healthcare and social media, managed big data frameworks (e.g, Spark,
Giraph, and Flink) require large amount of DRAM. 
These frameworks, during processing, generate large amount of objects
in the managed heap that span multiple computation stages.
%memory ressure -> GC
%large part of CPU utilization goes to GC.
%not useful work


The lack of memory when using these frameworks leads to long
garbage collection (GC) cycles, 

This
requirement leads to excessive CPU under-utilization, leading to
performance bottlenecks and slow job completion times.
\note{jk: explain why you only focus on the memory limit}

The memory limit problem can better be understood when looking at
analyses of traces from big companies e.g. Alibaba \cite{Alibaba}.
When looking at the results of this analysis, we see that memory usage
is at an average of 80\%, while CPU usage stays at 40\%. 

To address the challenges of lack of system memory, researchers have
proposed various techniques to optimize the performance of
applications, including utilizing remote memory and memory scheduling.

To address the problem of DRAM capacity limitation, recent work
proposed solutions that extend the managed heaps over local flash
storage devices (e.g., NVMe SSD) or remote memory. On the one hand,
TMO alalal. On the other hand,  

TMO offloads cold memory 


Some Meta researchers built a new memory scheduling mechanism to
investigate the oportunities for reducing memory pressure and improve
server throughput in their work called Transparent Memory Offloading
in Datacenters (TMO) \cite{TMO}. Amaro et. al investigate the
utilization of remote memory to improve job throughput \cite{CFM}
building a swapping mechnism that uses memory from remote servers as
swap memory.

This paper provides an \note{jk: methodology..} analysis of server throughput for managed big
data analytics frameworks. 

when using heap offloading to fast local storage devices.
Specifically, in order to investigate the heap offloading to fast
storage, we use TeraHeap \cite{TeraHeap}, a secondary managed
memory-mapped heap over an NVMe storage device, which is used to hold
the long lived objects instead of the main managed Java Heap. TeraHeap
1) eliminates Serialization/Deserialization overheads posed by this
kind of frameworks when moving data off-heap to/from fast storage
devices 2) reduces GC pauses drastically over the secondary heap. By
offloading the managed Java Heap and relaxing computation-intensive
tasks, we aim to reduce the load of Garbage Collection on executors,
thereby improving their performance and job throughput. We investigate
the effectiveness of heap offloading to fast storage using various big
data analytics workloads on a real-world cluster. We choose the
parameters for running the workloads by investigating the needs of big
data analytics in terms of memory budget. We define memory budget as
the summary of Java Heap, IO Cache and JVM native memory. We compare
TeraHeap with the native Spark and Giraph distributions and show that
an approach like TeraHeap could be used instead of this distribution
to improve performance and server throughput.

Our experimental results show that reducing Java Garbage Collection
overhead by offloading the heap to fast storage devices significantly
improves server throughput by up to 66\% against native Spark and
Giraph. Finally, we also include a cost estimation to show that
TeraHeap could reduce monetary cost by up to 50\% for running big data
analytics, if deployed in a world cluster like Amazon's EC2 or Google
Cloud Platform or Microsoft Azure Cloud, which are available to
everyone.

The paper makes the following contributions: 
\begin{itemize}
    \item{A detailed methodology for running Apache Spark and Giraph using TeraHeap
	  as a heap offloading mechanism.}
    \item{A comprehensive evaluation of the performance and cost trade-offs of TeraHeap against Native Spark-Giraph
	  with a single or multiple colocated executors in the same server.
    \item{A cost estimation of running our experiments in real-world cloud platforms like Amazon EC2, Google Cloud and Microsoft Azure.}
\end{itemize}
