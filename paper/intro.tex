\section{Introduction}
\label{sec:intro}

With the exponential growth of data in various fields such as finance,
healthcare, social media, and e-commerce, there is a significant need
for more and more system memory from datacenters. In order
to process and analyze these data loads, big data analytics frameworks
are needed e.g. Apache Spark \cite{Spark} and Giraph. 
The lack of memory when using these kind of frameworks leads to
long Garbage Collection cycles performed by the Java Virtual Machine.
This requirement leads to excessive CPU under-utilization, 
leading to performance bottlenecks and slow job completion
times.
\note{jk: explain why you only focus on the memory limit}
\par The memory limit problem can better be understood when looking at analyses of traces from big companies e.g. Alibaba \cite{Alibaba}.
When looking at the results of this analysis, we see that memory usage is at an average of 80\%, while CPU usage stays at 40\%. 
\par To address the challenges of lack of system memory, researchers have proposed various
techniques to optimize the performance of applications, including
utilizing remote memory and memory scheduling.
Some Meta researchers built a new memory scheduling mechanism to investigate the oportunities
for reducing memory pressure and improve server throughput in their work called Transparent Memory Offloading in Datacenters (TMO)
\cite{TMO}. Amaro et. al investigate the utilization of remote memory to improve job throughput \cite{CFM} building a swapping mechnism that uses memory from remote servers as swap memory.
\par Having taken into consideration the previous works in this field,
in this paper, we conduct an analysis of server throughput for big data analytics when using heap offloading to fast local storage devices.
Specifically, in order to investigate the heap offloading to fast storage,
we use TeraHeap \cite{TeraHeap}, a secondary managed
memory-mapped heap over an NVMe storage device, which is used to hold
the long lived objects instead of the main
managed Java Heap.
TeraHeap 1) eliminates Serialization/Deserialization overheads posed
by this kind of frameworks when moving data off-heap to/from fast
storage devices 2) reduces GC pauses drastically over the secondary heap.
By offloading the managed Java Heap and relaxing computation-intensive tasks, we aim
to reduce the load of Garbage Collection on executors, thereby improving their
performance and job throughput.
We investigate the effectiveness of heap offloading to fast storage using various big
data analytics workloads on a real-world cluster. We choose the parameters for running the workloads
by investigating the needs of big data analytics in terms of memory budget. We define memory budget as the summary of
Java Heap, IO Cache and JVM native memory. We
compare TeraHeap with the native Spark and Giraph distributions and show that
an approach like TeraHeap could be used instead of this distribution to improve
performance and server throughput.
\par Our experimental results show that reducing Java Garbage Collection overhead by offloading the heap to fast storage devices
significantly improves server throughput by up to 66\% against native Spark and Giraph. Finally, we also include
a cost estimation to show that TeraHeap could reduce monetary cost by up to 50\% for running big data analytics, if
deployed in a world cluster like Amazon's EC2 or Google Cloud Platform or Microsoft Azure Cloud, which are available to everyone.

The paper makes the following contributions: 
\begin{itemize}
    \item{A detailed methodology for running Apache Spark and Giraph using TeraHeap
	  as a heap offloading mechanism.}
    \item{A comprehensive evaluation of the performance and cost trade-offs of TeraHeap against Native Spark-Giraph
	  with a single or multiple colocated executors in the same server.
    \item{A cost estimation of running our experiments in real-world cloud platforms like Amazon EC2, Google Cloud and Microsoft Azure.}
\end{itemize}
